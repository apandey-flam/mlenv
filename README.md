# MLEnv - ML Environment Manager

![Version](https://img.shields.io/badge/version-1.0.0-blue)
![License](https://img.shields.io/badge/license-MIT-green)
![Platform](https://img.shields.io/badge/platform-Linux-lightgrey)
![NVIDIA](https://img.shields.io/badge/NVIDIA-GPU-76B900?logo=nvidia)
![Docker](https://img.shields.io/badge/Docker-20.10+-2496ED?logo=docker&logoColor=white)
![Bash](https://img.shields.io/badge/Bash-4EAA25?logo=gnubash&logoColor=white)
![CUDA](https://img.shields.io/badge/CUDA-Enabled-76B900)

**Production-Ready GPU Container Management**

A production-grade command-line tool for managing NVIDIA GPU-accelerated Docker containers for deep learning and scientific computing. Simplifies the workflow of running PyTorch, TensorFlow, and other GPU workloads while keeping your code safely on the host machine.

## ğŸš€ Features

- **Zero-Config GPU Access** - Automatic NVIDIA GPU detection and passthrough
- **Smart Requirements Management** - Hash-based caching prevents redundant pip installs
- **Persistent Workspaces** - Your code stays on the host (bind-mounted)
- **Port Forwarding** - Easy access to Jupyter, TensorBoard, APIs
- **Resource Controls** - Limit CPU, memory, and GPU usage
- **Multi-Project Support** - Unique container names prevent collisions
- **User Mapping** - Run as your user, not root (no permission issues)
- **One-Line Commands** - `mlenv jupyter`, `mlenv exec -c "train.py"`
- **Auto-Restart** - Containers survive system reboots

## ğŸ“‹ Prerequisites

- **Docker** (version 20.10+)
- **NVIDIA GPU** with compatible drivers
- **NVIDIA Container Toolkit** ([installation guide](https://github.com/NVIDIA/nvidia-container-toolkit))

Verify your setup:
```bash
docker run --rm --gpus all nvidia/cuda:12.0.0-base-ubuntu22.04 nvidia-smi
```

## ğŸ”§ Installation

### Automatic Installation (Recommended)
```bash
# Clone the repository
git clone https://github.com/your-username/mlenv.git
cd mlenv

# Run installer (checks prerequisites, installs script + completions)
sudo ./install.sh

# Verify installation
mlenv help
```

The installer will:
- âœ… Check Docker and NVIDIA Container Toolkit
- âœ… Install NGC to `/usr/local/bin`
- âœ… Install shell completions (bash/zsh/fish)
- âœ… Test GPU access

### Installation Options
```bash
# Check prerequisites without installing
./install.sh --check

# Install to custom directory (no sudo needed)
./install.sh --install-dir ~/.local/bin

# Force reinstall
sudo ./install.sh --force

# Uninstall
sudo ./install.sh --uninstall
```

### Manual Installation
```bash
# Download and make executable
chmod +x ngc

# Copy to system directory
sudo cp ngc /usr/local/bin/

# Or use without sudo (add to PATH)
mkdir -p ~/.local/bin
cp ngc ~/.local/bin/
echo 'export PATH="$HOME/.local/bin:$PATH"' >> ~/.bashrc
source ~/.bashrc
```

## âš¡ Quick Start

### Basic Usage
```bash
# Start a container
mlenv up

# Enter interactive shell
mlenv exec

# Run your code (inside container)
python train.py

# Exit (Ctrl+D or type 'exit')
# Container keeps running in background

# Stop when done
mlenv down
```

### For Private NGC Images
If you need private container images from NGC:

```bash
# 1. Get API key from https://ngc.nvidia.com/setup/api-key

# 2. Authenticate
mlenv login

# 3. Use private images
mlenv up --image nvcr.io/your-org/your-private-image:latest
```

**Note:** Public images like `nvcr.io/nvidia/pytorch:25.12-py3` don't require authentication.

### With Jupyter Lab
```bash
# Start with port forwarding
mlenv up --port 8888:8888

# Launch Jupyter
mlenv jupyter

# Open the URL shown in terminal
# http://localhost:8888/...
```

### With Requirements
```bash
# requirements.txt
# torch==2.1.0
# transformers==4.35.0
# jupyter==1.0.0

mlenv up --requirements requirements.txt --port 8888:8888
mlenv exec
```

## ğŸ“š Usage

### Commands

| Command | Description |
|---------|-------------|
| `mlenv up` | Create/start container |
| `mlenv exec` | Open interactive shell |
| `mlenv down` | Stop container |
| `mlenv restart` | Restart container |
| `mlenv rm` | Remove container (keeps code) |
| `mlenv status` | Show container and GPU status |
| `mlenv jupyter` | Start Jupyter Lab |
| `mlenv logs` | View debug logs |
| `mlenv clean` | Remove NGC artifacts |
| `mlenv help` | Show detailed help |

### Options for `mlenv up`

```bash
mlenv up [OPTIONS]

Options:
  --image <name>              Docker image (default: nvcr.io/nvidia/pytorch:25.12-py3)
  --requirements <file>       Install Python packages from file
  --force-requirements        Force reinstall even if cached
  --port <mapping>            Port forwarding (e.g., "8888:8888" or "8888:8888,6006:6006")
  --gpu <devices>             GPU devices (e.g., "0,1" or "all", default: all)
  --env-file <file>           Load environment variables from file
  --memory <limit>            Memory limit (e.g., "16g", "32g")
  --cpus <limit>              CPU limit (e.g., "4.0", "8.0")
  --no-user-mapping           Run as root instead of current user
  --verbose                   Enable verbose output
```

### Options for `mlenv exec`

```bash
mlenv exec [-c <command>]

Options:
  -c <command>                Execute command instead of interactive shell

Examples:
  mlenv exec                            # Interactive shell
  mlenv exec -c "python train.py"       # Run training script
  mlenv exec -c "pip list | grep torch" # Check installed packages
```

## ğŸ’¡ Examples

### Example 1: PyTorch Deep Learning
```bash
# requirements.txt
torch==2.1.0
torchvision==0.16.0
tensorboard==2.15.0
wandb==0.16.0

# .env
WANDB_API_KEY=your_api_key_here
CUDA_VISIBLE_DEVICES=0,1

# Setup environment
mlenv up \
  --requirements requirements.txt \
  --env-file .env \
  --port 6006:6006 \
  --gpu 0,1 \
  --memory 32g

# Start training
mlenv exec -c "python train.py --batch-size 64 --epochs 100"

# Monitor with TensorBoard (in another terminal)
mlenv exec -c "tensorboard --logdir runs --host 0.0.0.0"
# Open: http://localhost:6006
```

### Example 2: Jupyter Notebook Development
```bash
# Start container with Jupyter port
mlenv up --requirements requirements.txt --port 8888:8888

# Launch Jupyter Lab
mlenv jupyter

# Open the provided URL in your browser
# Notebooks saved in /workspace are automatically synced to host
```

### Example 3: Distributed Training (Multi-GPU)
```bash
# Use all available GPUs
mlenv up --requirements requirements.txt --gpu all

# Run distributed training
mlenv exec -c "torchrun --nproc_per_node=4 train_ddp.py --config config.yaml"
```

### Example 4: Model Inference API
```bash
# requirements.txt
fastapi==0.104.0
uvicorn==0.24.0
torch==2.1.0
transformers==4.35.0

# Start with API port
mlenv up --requirements requirements.txt --port 8000:8000 --gpu 0

# Run FastAPI server
mlenv exec -c "uvicorn app:app --host 0.0.0.0 --port 8000"

# API available at http://localhost:8000
```

### Example 5: Data Processing (CPU-Heavy)
```bash
# requirements.txt
pandas==2.1.0
polars==0.19.0
dask==2023.10.0

# High CPU/memory, no GPU needed
mlenv up \
  --requirements requirements.txt \
  --memory 64g \
  --cpus 16.0 \
  --gpu 0  # Use only one GPU or none

# Process large dataset
mlenv exec -c "python process_data.py --input data/raw --output data/processed"
```

### Example 6: Custom Image (TensorFlow)
```bash
# Use TensorFlow image instead of PyTorch
mlenv up \
  --image nvcr.io/nvidia/tensorflow:24.12-tf2-py3 \
  --requirements requirements.txt \
  --port 8888:8888

mlenv exec
```

## ğŸ” NGC Authentication

### When Do You Need It?

**Public Images** - No authentication needed:
- `nvcr.io/nvidia/pytorch:*`
- `nvcr.io/nvidia/tensorflow:*`
- `nvcr.io/nvidia/cuda:*`

**Private Images** - Authentication required:
- `nvcr.io/your-org/your-private-model:*`
- Organization-specific images
- Enterprise NGC images

### Setup Authentication

```bash
# 1. Get your NGC API Key
# Visit: https://ngc.nvidia.com/setup/api-key
# Click "Generate API Key"

# 2. Login to NGC
mlenv login
# Paste your API key when prompted

# 3. Verify authentication
docker info | grep nvcr.io
# Should show: Username: $oauthtoken

# 4. Use private images
mlenv up --image nvcr.io/your-org/private-model:latest
```

### What Gets Stored?

```bash
~/.mlenv/config           # NGC API key
~/.docker/config.json   # Docker registry credentials
```

### Logout

```bash
mlenv logout
# Removes NGC credentials and Docker login
```

### Troubleshooting

**"unauthorized: authentication required"**
```bash
# Not logged in, run:
mlenv login
```

**"Error response from daemon: Get https://nvcr.io/v2/: unauthorized"**
```bash
# Expired/invalid API key
mlenv logout
mlenv login  # Enter new key
```

**Check if logged in:**
```bash
cat ~/.mlenv/config
# or
docker info | grep nvcr.io
```

## ğŸ” How It Works

### Architecture
```
Host Machine                    Docker Container
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”            â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚  /your/project  â”‚â—„â”€â”€bindâ”€â”€â”€â”€â–ºâ”‚   /workspace    â”‚
â”‚                 â”‚   mount    â”‚                 â”‚
â”‚  â”œâ”€â”€ train.py   â”‚            â”‚  â”œâ”€â”€ train.py   â”‚
â”‚  â”œâ”€â”€ data/      â”‚            â”‚  â”œâ”€â”€ data/      â”‚
â”‚  â””â”€â”€ .mlenv/      â”‚            â”‚  â””â”€â”€ (GPUs)     â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜            â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
        â–²                               â”‚
        â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
                       â”‚
                  localhost:8888
```

### Key Concepts

1. **Bind Mounting**: Your project directory is mounted into the container at `/workspace`. Changes are immediately synced both ways.

2. **Container Persistence**: Containers stay running in the background. Stop with `mlenv down`, remove with `mlenv rm`.

3. **Smart Caching**: Requirements are hashed. Reinstalls only happen if the file changes (override with `--force-requirements`).

4. **Unique Naming**: Container names include a directory hash (`ngc-myproject-a3f8c21d`) to prevent collisions across different project directories.

5. **User Mapping**: By default, runs as your user (`uid:gid`) to avoid permission issues with created files.

## ğŸ› ï¸ Advanced Usage

### Multiple Projects
Each directory gets its own container:
```bash
cd /projects/nlp-research
mlenv up  # Creates: ngc-nlp-research-abc123

cd /projects/computer-vision  
mlenv up  # Creates: ngc-computer-vision-def456

# Both can run simultaneously
```

### Custom Docker Images
```bash
# NVIDIA PyTorch (default)
mlenv up --image nvcr.io/nvidia/pytorch:25.12-py3

# NVIDIA TensorFlow
mlenv up --image nvcr.io/nvidia/tensorflow:24.12-tf2-py3

# Custom image with your base setup
mlenv up --image yourusername/ml-base:latest
```

### Environment Variables
```bash
# .env file
API_KEY=secret123
MODEL_PATH=/workspace/models
BATCH_SIZE=32
WANDB_PROJECT=my-experiment

mlenv up --env-file .env

# Access inside container
mlenv exec -c 'echo $API_KEY'
```

### Resource Management
```bash
# Limit resources to share GPU server
mlenv up \
  --gpu 0,1 \        # Use only GPUs 0 and 1
  --memory 32g \     # Max 32GB RAM
  --cpus 8.0         # Max 8 CPU cores
```

### Development + Production Setup
```bash
# Development (with Jupyter, TensorBoard)
mlenv up \
  --requirements requirements-dev.txt \
  --port 8888:8888,6006:6006 \
  --verbose

# Production (lean, specific resources)
mlenv up \
  --requirements requirements.txt \
  --gpu 0 \
  --memory 16g \
  --no-user-mapping  # If needed for deployment
```

## ğŸ“Š Monitoring & Debugging

### Check Container Status
```bash
mlenv status

# Output:
# â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”
# Container: ngc-myproject-a3f8c21d
# Status: running
# Workdir: /home/user/projects/myproject
# â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”
#
# GPU Status:
# index, name, utilization.gpu, memory.used, memory.total
# 0, NVIDIA A100, 45%, 12000 MiB, 40960 MiB
```

### View Logs
```bash
# NGC manager logs
mlenv logs

# Docker container logs
docker logs ngc-myproject-a3f8c21d

# Follow logs in real-time
docker logs -f ngc-myproject-a3f8c21d
```

### GPU Monitoring
```bash
# Inside container
mlenv exec -c "watch -n 1 nvidia-smi"

# Or use gpustat
mlenv exec -c "pip install gpustat && gpustat -i 1"
```

## ğŸ› Troubleshooting

### Container won't start
```bash
# Check Docker daemon
docker info

# Check NVIDIA runtime
docker info | grep -i nvidia

# View detailed logs
mlenv up --verbose
mlenv logs
```

### GPU not detected
```bash
# Verify NVIDIA driver
nvidia-smi

# Test GPU in Docker
docker run --rm --gpus all nvidia/cuda:12.0.0-base-ubuntu22.04 nvidia-smi

# If failed, reinstall NVIDIA Container Toolkit:
# https://docs.nvidia.com/datacenter/cloud-native/container-toolkit/install-guide.html
```

### Port already in use
```bash
# Find what's using the port
lsof -ti:8888

# Kill the process
lsof -ti:8888 | xargs kill -9

# Or use a different port
mlenv up --port 8889:8888
```

### Permission denied errors
```bash
# Files created in container have wrong ownership?
# By default runs as your user, but if you need root:
mlenv up --no-user-mapping

# Fix existing permissions
mlenv exec -c "chown -R $(id -u):$(id -g) /workspace"
```

### Requirements won't update
```bash
# Force reinstall
mlenv up --force-requirements

# Or remove and recreate
mlenv rm
mlenv up --requirements requirements.txt
```

### Out of memory
```bash
# Check GPU memory
mlenv exec -c "nvidia-smi"

# Increase shared memory (edit script line 83):
--shm-size=32g  # default is 16g

# Or reduce batch size in your code
```

### Container name collision
The script uses directory hash to prevent this, but if you renamed your project:
```bash
# List all NGC containers
docker ps -a | grep ngc-

# Remove old ones
docker rm -f ngc-oldname-12345678
```

## ğŸ”’ Security Best Practices

1. **Use `.gitignore`**:
```bash
# .gitignore
.mlenv/
.env
*.pth
*.ckpt
```

2. **Environment Variables**: Never commit API keys. Use `--env-file` with `.env` in `.gitignore`.

3. **User Mapping**: Default runs as your user (not root). Only use `--no-user-mapping` when necessary.

4. **Network Isolation**: Ports are only exposed on localhost by default. For remote access, use SSH tunneling:
```bash
# On remote server
mlenv up --port 8888:8888

# On local machine
ssh -L 8888:localhost:8888 user@remote-server

# Access via localhost:8888
```

## ğŸ¯ Best Practices

### Project Structure
```
my-ml-project/
â”œâ”€â”€ ngc                 # This script
â”œâ”€â”€ requirements.txt    # Python dependencies
â”œâ”€â”€ .env               # Environment variables (gitignored)
â”œâ”€â”€ .gitignore         # Ignore .mlenv/, .env, checkpoints
â”œâ”€â”€ train.py           # Training script
â”œâ”€â”€ evaluate.py        # Evaluation script
â”œâ”€â”€ data/              # Dataset
â”œâ”€â”€ models/            # Saved models
â”œâ”€â”€ notebooks/         # Jupyter notebooks
â””â”€â”€ .mlenv/              # NGC logs (gitignored)
    â””â”€â”€ ngc.log
```

### Workflow
```bash
# 1. Setup (once per project)
mlenv up --requirements requirements.txt --port 8888:8888

# 2. Develop
mlenv jupyter  # or mlenv exec

# 3. When done for the day
mlenv down

# 4. Next day
mlenv up  # Fast startup, requirements cached
mlenv exec

# 5. Clean slate (if needed)
mlenv rm
mlenv up --force-requirements
```

### Version Control
```bash
# Commit the script with your project
git add mlenv requirements.txt
git commit -m "Add NGC container manager"

# Others can use it
git clone <your-repo>
cd <your-repo>
chmod +x ngc
./mlenv up --requirements requirements.txt
```

## ğŸš§ Known Limitations

Current version has a few limitations we're aware of:

- **No version command** - ~~Can't check installed version~~ âœ… Added in v1.0.0
- **Manual updates** - No built-in update mechanism (requires git pull + reinstall)
- **No config file** - Can't set persistent defaults (`.ngcrc` planned)
- **Basic cleanup** - ~~Old containers need manual removal~~ âœ… Added `mlenv clean --containers` in v1.0.0
- **Manual completion reload** - Shell completions require terminal restart

**Note:** This tool is designed for Linux systems with NVIDIA GPUs. macOS and Windows native are not supported. Windows users should use WSL2.

See the [Roadmap](#-roadmap) below for planned improvements.

## ğŸ—ºï¸ Roadmap

### v1.1 (Next Release)
- [ ] **Auto-update mechanism** - `mlenv update` to pull latest version
- [ ] **Container listing** - ~~`mlenv list` to show all NGC containers~~ âœ… Added in v1.0.0
- [ ] **Enhanced status** - Show resource usage (CPU, memory, GPU utilization)
- [ ] **Better error messages** - Contextual help and suggestions

### v1.2 (Planned)
- [ ] **Config file support** - `~/.ngcrc` for default settings
- [ ] **Project templates** - `mlenv init --template pytorch|tensorflow|transformers`
- [ ] **Auto GPU detection** - `mlenv up --auto-gpu` to find free GPUs
- [ ] **Better testing** - Full integration test suite

### v2.0 (Future)
- [ ] **VS Code integration** - Auto-generate `.devcontainer.json`
- [ ] **Multi-container** - Support for related services (db, web, training)
- [ ] **Experiment tracking** - Built-in W&B, MLflow integration
- [ ] **GPU scheduling** - Wait for GPU availability
- [ ] **Jupyter extensions** - Auto-install popular extensions
- [ ] **Central management** - Team dashboard for shared servers
- [ ] **Container snapshots** - Save/restore container state

### Want a Feature?
Open an issue describing your use case, or submit a PR! Popular requests get prioritized.

## ğŸ¤ Contributing

Contributions are welcome! Please feel free to submit a Pull Request.

### Development Setup
```bash
git clone https://github.com/your-username/mlenv.git
cd mlenv
chmod +x ngc

# Test locally
./mlenv up --verbose
./mlenv status
```

### Priority Areas
We'd especially appreciate help with:
- **Windows WSL2** - Testing and documentation improvements
- **Shell completions** - Improvements for zsh/fish
- **Test coverage** - More comprehensive test suite
- **Documentation** - Examples for specific frameworks (Transformers, Stable Diffusion, etc.)

## ğŸ“„ License

MIT License - see LICENSE file for details

## ğŸ™ Acknowledgments

- Built on top of [NVIDIA NGC containers](https://catalog.ngc.nvidia.com/)
- Inspired by Docker Compose and development container workflows
- Uses [NVIDIA Container Toolkit](https://github.com/NVIDIA/nvidia-container-toolkit)

## ğŸ“ Support

- **Issues**: [GitHub Issues](https://github.com/your-username/mlenv/issues)
- **Discussions**: [GitHub Discussions](https://github.com/your-username/mlenv/discussions)
- **Documentation**: See `QUICKSTART.md` and `IMPROVEMENTS.md`

## ğŸ”— Related Projects

- [nvidia-docker](https://github.com/NVIDIA/nvidia-docker) - NVIDIA Container Toolkit
- [docker-compose](https://docs.docker.com/compose/) - Multi-container orchestration
- [Dev Containers](https://containers.dev/) - VS Code development containers

---

**Made with â¤ï¸ for the ML/DL community**